# -*- coding: utf-8 -*-
"""Cleaning_Skript.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-6c5Kx90GlPGadiCnszs4QRPrwOrdeva
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

def handle_outliers(df):

    # checking for outliers in numeric columns with IQR
    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

    for col in numeric_cols:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        # I put a tolerance value, so not every outlier getting cleared
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # replace outliers with NaNs
        df[col] = df[col].apply(
            lambda x: x if (x >= lower_bound and x <= upper_bound) else None
        )

    return df

def missing_values(df):
  num_cols = df.select_dtypes(include=['float64', 'int64']).columns
  cat_cols = df.select_dtypes(include=['object', 'string', 'category']).columns
  missing = df.isnull().sum()
  missing_total = missing.sum()
  print(f"Total missing values: {missing_total}.")

  Rows = len(df)
  nan_threshold = 0.4
  for col in num_cols:
    NaNs = df[col].isnull().sum()
    print(f"Before cleaning there are {NaNs} missing values in {col}.")
    mean = df[col].mean()
    if NaNs/Rows >= nan_threshold:
      print(f"Mean of {col} is {mean}.")
      df = df.drop(columns=[col])
    else:
      df[col] = df[col].fillna(mean)

  for col in cat_cols:
    NaNs = df[col].isnull().sum()
    print(f"Before cleaning there are {NaNs} missing values in {col}.")
    mode_series = df[col].mode()
    mode_value = mode_series.values[0] if len(mode_series) > 0 else None
    if NaNs/Rows >= nan_threshold:
      df = df.drop(columns=[col])
    else:
      print(f"The most used value of {col} is {mode_value}.")
      df[col] = df[col].fillna(mode_value)

  return df

def check_and_remove_duplicates(df):

    # counts the number of duplicates
    duplicates_count = df.duplicated().sum()
    print(f"Found duplicates: {duplicates_count}")

    # drops the duplicates and prints the change from before and after cleaning
    df = df.drop_duplicates()

    print(f"After Cleaning: {len(df)} rows (before {len(df)})")

    return df

def check_and_fix_dtypes(df):

    print("Before cleaning:")
    print(df.dtypes)

    # checking for wrong data types
    for col in df.columns:
        # if a column has object as dtype, it tries to change to numeric
        # 'raise' causes a error if this is not possible and the data type
        # remains an object
        if df[col].dtype == 'object':
            try:
                df[col] = pd.to_numeric(df[col], errors='raise')
                print(f"Column '{col}' from object → numeric converted.")
            except:
                try:
                    # if the dtype is an object and no numeric column it also
                    # tries to change the object to datetime
                    # if the column is a categorical column 'raise' prevent a
                    # change and the column remains an object
                    df[col] = pd.to_datetime(df[col], errors='raise')
                    print(f"Column '{col}' from object → datetime converted.")
                except:
                    pass

    print("\nAfter cleaning:")
    print(df.dtypes)

    return df

def scale_and_encode(df):

    # scaling numeric columns with the StandardScaler because this is a dataset
    # for a Machine Learning Model
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    scaler = StandardScaler()
    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    print(f"Scaled numeric columns: {list(numeric_cols)}")

    # transform categorical columns with the OneHotEncoder in numbers to make
    # the columns useable for Machine Learning
    categorical_cols = df.select_dtypes(include=['object']).columns
    encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop first, to prevent of redundancy
    ohe_data = encoder.fit_transform(df[categorical_cols])
    ohe_df = pd.DataFrame(ohe_data, columns=encoder.get_feature_names_out(categorical_cols), index=df.index)
    df = df.drop(categorical_cols, axis=1)
    df = pd.concat([df, ohe_df], axis=1)
    print(f"One-Hot-Encoded Columns: {list(categorical_cols)}")


    return df

def save_cleaned_data(df,filename):
  # changing the dataframe in a csv
  df.to_csv(filename, index=False)
  print(f"CSV-file '{filename}' is saved")

  return df

def cleaning_total_without_scaling_and_encoding(df,filename):
  df = handle_outliers(df)
  df = missing_values(df)
  df = check_and_remove_duplicates(df)
  df = check_and_fix_dtypes(df)
  df = save_cleaned_data(df,filename)

  return df

def cleaning_total_with_scaling_and_encoding(df,filename):
  df = handle_outliers(df)
  df = missing_values(df)
  df = check_and_remove_duplicates(df)
  df = check_and_fix_dtypes(df)
  df = scale_and_encode(df)
  df = save_cleaned_data(df,filename)

  return df